{
  "metadata": {
    "name": "SCR Data v2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SCR Data Profile\n\u003e Kamiku Xue(yx3494@nyu.edu)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// define the project root\nimport org.apache.spark.sql.DataFrame\nval root_folder \u003d \"/user/yx3494_nyu_edu/scr_data/\"\nvar year \u003d 2018 to 2023"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1 BOCES and Need-to-Resource Capacity Categories(N/RC)\n\nThe need-to-resource capacity (N/RC) index, a measure of a districtâ€™s ability to meet the needs of its students with local\nresources, is the ratio of the estimated poverty percentage1 (expressed in standard score form) to the Combined Wealth\nRatio2 (expressed in standard score form). A district with both estimated poverty and Combined Wealth Ratio equal to\nthe State average would have a N/RC index of 1.0. N/RC categories are determined from this index using the definitions\nin the table below."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// First see the data schema\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n\n        println(\"Year \" + i)\n        println(\"Total Columns \" + df.columns.length)\n        \n        // see data structure\n        df.printSchema\n        // peek some data\n        z.show(df.limit(3))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nWe will use the following columns:\n\n- `ENTITY_CD `: Unique identifier for the entity for foreign key\n- `SCHOOL_NAME`: The name of the school\n- `YEAR`: School Year (2021 for 2020-21, 2022 for 2021-22, 2023 for 2022-23)\n- `DISTRICT_NAME`: The name of the district\n- `COUNTY_NAME`: The name of the county\n- `NEEDS_INDEX`: N/RC index\n\nNow profile the each column"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// loop from 2018 to 2023\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n\n        println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n        println(\"Total entities \" + df.count)\n\n        // profile the YEAR\n        println(\"YEAR Profile\")\n        z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\")) // -\u003e 3, this explains some entities occur more than once, because they are in different years\n\n\n        // profile the ENTITY_CD\n        println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n        println(\"empty, null values: \" + df.filter(\"ENTITY_CD is null or ENTITY_CD \u003d \u0027\u0027\").count)\n        val groups_ids \u003d df.groupBy(\"ENTITY_CD\").count()\n        z.show(groups_ids.select(\"count\").describe()) // max: 3, min: 2, avg: 2.99 -\u003e most of the entities occur 3 times\n\n        // profile the SCHOOL_NAME\n        println(\"SCHOOL_NAME Profile, distinct schools:\" + df.select(\"SCHOOL_NAME\").distinct.count)\n        val groups_schools \u003d df.groupBy(\"SCHOOL_NAME\").count()\n        z.show(groups_schools.describe()) // some values occur 12 times, no empty names\n\n        // need to bind the SCHOOL_NAME with ENTITY_CD to check if the same school has different entity code\n        println(\"CHOOL_NAME + ENTITY_CD Profile, Distribution of entities\")\n        val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"SCHOOL_NAME\").count().select(\"count\")\n        z.show(groups_schools_entity.describe()) // max: 3, min: 2, avg: 2.99 -\u003e match the entity_cd\n\n        // profile the DISTRICT_NAME\n        println(\"DISTRICT_NAME Profile, unique districts: \" + df.select(\"DISTRICT_NAME\").distinct.count)\n        \n        //group by YEAR for the NEEDS_INDEX\n        val groups_districts \u003d df.groupBy(\"DISTRICT_NAME\").count()\n        z.show(groups_districts.describe()) //DISTRICT max occur 800, min 5 times\n\n        // profile the NEEDS_INDEX\n        println(\"NEEDS_INDEX Profile\")\n        z.show(df.describe(\"NEEDS_INDEX\")) // max:7, min: 1, avg: 3.55, stddev: 2.09\n        \n        //differset school year NEEDS_INDEX distribution\n        println(\"NEEDS_INDEX Profile in different years\")\n        z.show(df\n        .groupBy(\"YEAR\")\n        .agg(\n            sum(\"NEEDS_INDEX\"),\n            avg(\"NEEDS_INDEX\"), \n            min(\"NEEDS_INDEX\"), \n            max(\"NEEDS_INDEX\"), \n            stddev(\"NEEDS_INDEX\"))\n            .orderBy(\"YEAR\"))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n### N/RC Clean Step\n\nWe will do the following steps to clean the data"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// create a dataframe to store the data for all years\nvar nrcDF : DataFrame \u003d null\n\n// UDF for need index description\nval getNeedIndex \u003d (index: Int) \u003d\u003e {\n    index match {\n    case 1 \u003d\u003e \"High N/RC: New York City\"\n    case 2 \u003d\u003e \"High N/RC: Large City Districts \"\n    case 3 \u003d\u003e \"High N/RC: Urban-Suburban Districts\"\n    case 4 \u003d\u003e \"High N/RC: Rural Districts\"\n    case 5 \u003d\u003e \"Average N/RC Districts\"\n    case 6 \u003d\u003e \"Low N/RC Districts\"\n    case 7 \u003d\u003e \"Charter Schools\"\n    }\n}\nspark.udf.register(\"nrcStr\", getNeedIndex)\n\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n        // filder other year\u0027s data\n        .filter(\"YEAR \u003d \" + i)\n        // drop null values in SCHOOL_NAME\n        .filter(\"SCHOOL_NAME is not null and SCHOOL_NAME !\u003d \u0027\u0027\")\n        // in DISTRICT_NAME, if null, replace with \u0027UNAVAILABLE\u0027\n        .withColumn(\"DISTRICT_NAME\", when(col(\"DISTRICT_NAME\").isNull, \"UNAVAILABLE\").otherwise(col(\"DISTRICT_NAME\")))\n        .withColumn(\"NEEDS_DESCRIPTION\", expr(\"nrcStr(NEEDS_INDEX)\"))\n        // only select the columns we need\n        .select(\"ENTITY_CD\", \"SCHOOL_NAME\", \"YEAR\", \"NEEDS_INDEX\", \"NEEDS_DESCRIPTION\")\n        // rename the columns\n        .withColumnRenamed(\"ENTITY_CD\", \"School_BEDS_Code\")\n        .withColumnRenamed(\"SCHOOL_NAME\", \"School_Name\")\n        .withColumnRenamed(\"YEAR\", \"Year\")\n        .withColumnRenamed(\"NEEDS_INDEX\", \"N/RC_Index\")\n        .withColumnRenamed(\"NEEDS_DESCRIPTION\", \"N/RC_Index_Description\")\n\n\n    println(\"Year \" + i + \" (Total Entities: \" + df.count + \")\")\n    if (nrcDF \u003d\u003d null){\n        nrcDF \u003d df\n    } else {\n        nrcDF \u003d nrcDF.union(df)\n    }\n}\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final NR/C Dataframe (Total: \" + nrcDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\nz.show(nrcDF.limit(10))\n\n// save the dataframe\nnrcDF.write.mode(\"overwrite\").parquet(root_folder + \"nrc_cleaned.parquet\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2 Teaching Staff Data\n\nThe Teaching Staff data provides information on the number of teachers and principals have experience or inexperience in hight-proverty, low-performing schools.\n\n\u003e From 2020, the BOCES update the data version for Staff Qualiuifications, we need to check the columns when merge."
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2018 and 2019 have Staff_Qualifications.csv\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // show the schema\n    println(\"Columns: \" + df.columns.length)\n    df.printSchema\n    // peek some data\n    z.show(df.limit(3))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2020 - 2023 have Inexperienced_Teachers_Principals.csv\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // show the schema\n    println(\"Columns: \" + df.columns.length)\n    df.printSchema\n    // peek some data\n    z.show(df.limit(3))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We find some different columns in the teaching staff and qualifications data\n\n`INSTITUTION_ID, TOT_TEACH_LOW, TOT_TEACH_HIGH, TOT_PRINC_LOW, TOT_PRINC_HIGH, TEACH_DATA_REP_FLAG, PRIN_DATA_REP_FLAG`\n\nWe will not use above diff columns, we will use the following columns:\n\n- ENTITY_CD  - Unique identifier for the entity for foreign key\n- ENTITY_NAME - The name of the school / district\n- YEAR - School Year (etc. 2021 for 2020-21)\n- NUM_TEACH - Total number of teachers in the Student Information Repository System\n(SIRS)\n- NUM_TEACH_INEXP - Number of inexperienced teachers\n- NUM_TEACH_LOW - Number of teachers with low-poverty schools statewide\n- NUM_TEACH_HIGH - Number of teachers with high-poverty schools statewide\n- NUM_PRINC - Total number of principals\n- NUM_PRINC_INEXP - Number of inexperienced principals\n- NUM_ PRINC_LOW - Number of principals with low-poverty schools statewide\n- NUM_PRINC_HIGH - Number of principals with high-poverty schools statewide\n\nNext profile these data"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// for 2018 and 2019 Profile\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n    \n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    println(\"Total entities \" + df.count)    \n\n    // profile the YEAR\n    println(\"YEAR Profile\")\n    z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n\n    // profile the ENTITY_CD\n    println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n\n    // profile the ENTITY_NAME\n    println(\"ENTITY_NAME Profile, distinct schools:\" + df.select(\"ENTITY_NAME\").distinct.count)\n\n    // profile the ENTITY_CD + ENTITY_NAME\n    println(\"ENTITY_CD + ENTITY_NAME Profile, Distribution of entities\")\n    val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"ENTITY_NAME\").count().select(\"count\")\n    z.show(groups_schools_entity.describe())\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH Profile\")\n    z.show(df.describe(\"NUM_TEACH\")) \n\n    // profile the NUM_TEACH_INEXP\n    println(\"NUM_TEACH_INEXP Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH_LOW Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_PRINC\n    println(\"NUM_PRINC Profile\")\n    z.show(df.describe(\"NUM_PRINC\"))\n\n    // profile the NUM_PRINC_INEXP\n    println(\"NUM_PRINC_INEXP Profile\")\n    z.show(df.describe(\"NUM_PRINC_INEXP\"))\n\n    // profile the NUM_PRINC_LOW\n    println(\"NUM_PRINC_LOW Profile\")\n    z.show(df.describe(\"NUM_PRINC_LOW\"))\n\n    // profile the NUM_PRINC_HIGH\n    println(\"NUM_PRINC_HIGH Profile\")\n    z.show(df.describe(\"NUM_PRINC_HIGH\"))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// for 2020 and 2023 Inexperienced Teachers Principals\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n    \n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    println(\"Total entities \" + df.count)    \n\n    // profile the YEAR\n    println(\"YEAR Profile\")\n    z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n    // profile the ENTITY_CD\n    println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n\n    // profile the ENTITY_NAME\n    println(\"ENTITY_NAME Profile, distinct schools:\" + df.select(\"ENTITY_NAME\").distinct.count)\n\n    // profile the ENTITY_CD + ENTITY_NAME\n    println(\"ENTITY_CD + ENTITY_NAME Profile, Distribution of entities\")\n    val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"ENTITY_NAME\").count().select(\"count\")\n    z.show(groups_schools_entity.describe())\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH Profile\")\n    z.show(df.describe(\"NUM_TEACH\")) \n\n    // profile the NUM_TEACH_INEXP\n    println(\"NUM_TEACH_INEXP Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH_LOW Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_PRINC\n    println(\"NUM_PRINC Profile\")\n    z.show(df.describe(\"NUM_PRINC\"))\n\n    // profile the NUM_PRINC_INEXP\n    println(\"NUM_PRINC_INEXP Profile\")\n    z.show(df.describe(\"NUM_PRINC_INEXP\"))\n\n    // profile the NUM_PRINC_LOW\n    println(\"NUM_PRINC_LOW Profile\")\n    z.show(df.describe(\"NUM_PRINC_LOW\"))\n\n    // profile the NUM_PRINC_HIGH\n    println(\"NUM_PRINC_HIGH Profile\")\n    z.show(df.describe(\"NUM_PRINC_HIGH\"))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Teaching Staff Clean Step\n\nThis data is almost clean, just pick the columns needed and merge to one table"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// create a dataframe to store the data for 2018 and 2019\nvar oldStaffDF : DataFrame \u003d null\n\n// 2018 and 2019 have Staff_Qualifications.csv\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n        // select the current year data\n        .filter(\"YEAR \u003d \" + i)\n        // select the columns\n        .select(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\", \"NUM_TEACH\", \"NUM_TEACH_INEXP\", \"NUM_PRINC\", \"NUM_PRINC_INEXP\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    if (oldStaffDF \u003d\u003d null){\n        oldStaffDF \u003d df\n    } else {\n        oldStaffDF \u003d oldStaffDF.union(df)\n    }\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// create a dataframe to store the data for 2020 to 2023\nvar newStaffDF : DataFrame \u003d null\n// 2020 and 2023 have Inexperienced_Teachers_Principals.csv\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n        // select the current year data\n        .filter(\"YEAR \u003d \" + i)\n        // select the columns\n        .select(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\", \"NUM_TEACH\", \"NUM_TEACH_INEXP\", \"NUM_PRINC\", \"NUM_PRINC_INEXP\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    if (newStaffDF \u003d\u003d null){\n        newStaffDF \u003d df\n    } else {\n        newStaffDF \u003d newStaffDF.union(df)\n    }\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// final staff dataframe for 2018 - 2023\nval finalStaffDF \u003d oldStaffDF\n.union(newStaffDF)\n// rename the columns\n.withColumnRenamed(\"ENTITY_CD\", \"School_BEDS_Code\")\n.withColumnRenamed(\"ENTITY_NAME\", \"School_Name\")\n.withColumnRenamed(\"YEAR\", \"Year\")\n.withColumnRenamed(\"NUM_TEACH\", \"Total_Teachers\")\n.withColumnRenamed(\"NUM_TEACH_INEXP\", \"4-_years_Teachers\")\n.withColumnRenamed(\"NUM_PRINC\", \"Total_Principals\")\n.withColumnRenamed(\"NUM_PRINC_INEXP\", \"4-_years_Principals\")\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final Staff Dataframe (Total: \" + finalStaffDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n// peek some data row\nz.show(finalStaffDF.limit(10))\n// save the dataframe\nfinalStaffDF.write.mode(\"overwrite\").parquet(root_folder + \"staff_cleaned.parquet\")\n\nfinalStaffDF.printSchema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3 Graduation Rate (new)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "for (i \u003c- 2018 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Graduation_Rate.csv\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // show the schema\n    println(\"Columns: \" + df.columns.length)\n    df.printSchema\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Clean Data\n\n- Merget the 2018 -2023 data\n- Rename the coloums"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// define the root df\nvar gradDF : DataFrame \u003d null\nfor (i \u003c- 2018 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Graduation_Rate.csv\")\n        // filter the current year data\n        .filter(\"YEAR \u003d \" + i)\n        // select the columns we need\n        .select(\"YEAR\", \"ENTITY_CD\", \"ENTITY_NAME\", \"SUBGROUP_NAME\", \"COHORT\", \"COHORT_COUNT\", \"GRAD_RATE\")\n    if (gradDF \u003d\u003d null){\n        gradDF \u003d df\n    } else {\n        gradDF \u003d gradDF.union(df)\n    }\n}\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final Graduation Rate Dataframe (Total: \" + gradDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// get all entities combine grad rate\nval entityGradRate \u003d gradDF\n.filter(\"SUBGROUP_NAME \u003d \u0027All Students\u0027 AND COHORT \u003d \u0027Combined\u0027\")\n// if gard_rate is \u0027s\u0027, replace with 0 int\n.withColumn(\"GRAD_RATE\", when(col(\"GRAD_RATE\") \u003d\u003d\u003d \"s\", 0).otherwise(col(\"GRAD_RATE\").cast(\"int\")))\n.groupBy(\"ENTITY_NAME\")\n.agg(\n    min(\"GRAD_RATE\").alias(\"MIN\"),\n    max(\"GRAD_RATE\").alias(\"MAX\"),\n    avg(\"GRAD_RATE\").alias(\"AVG\"),\n    stddev(\"GRAD_RATE\").alias(\"STDDEV\")\n)\n\n// show the distribution, invse the order\nz.show(entityGradRate.orderBy(desc(\"AVG\")).limit(10))"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// per school per year grad rate\nval schoolYearGradRate \u003d gradDF\n.filter(\"SUBGROUP_NAME \u003d \u0027All Students\u0027 AND COHORT \u003d \u0027Combined\u0027 AND GRAD_RATE !\u003d \u0027s\u0027\")\n// if gard_rate is \u0027s\u0027, replace with 0 int\n.withColumn(\"GRAD_RATE\", when(col(\"GRAD_RATE\") \u003d\u003d\u003d \"s\", 0).otherwise(col(\"GRAD_RATE\").cast(\"int\")))\n.groupBy(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\")\n.agg(\n    avg(\"GRAD_RATE\").alias(\"Gruadation_Rate\")\n)\n// rename the column\n.withColumnRenamed(\"ENTITY_CD\", \"School_BEDS_Code\")\n.withColumnRenamed(\"ENTITY_NAME\", \"School_Name\")\n.withColumnRenamed(\"YEAR\", \"Year\")\n\n// show the distribution,invse the order\nz.show(schoolYearGradRate.orderBy(desc(\"Gruadation_Rate\")).orderBy(\"School_BEDS_Code\", \"Year\").limit(10))\nschoolYearGradRate.count\n\n// save the dataframe\ngradDF.write.mode(\"overwrite\").parquet(root_folder + \"grad_rate_cleaned.parquet\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Final Data Merge\n\nMerge the N/RC, Teaching Staff and Graduation Rate data to one table, also cast the Entity cd to the string for the foreign table join"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val combinedDF \u003d nrcDF\n.join(finalStaffDF, Seq(\"School_BEDS_Code\", \"School_Name\", \"Year\"), \"inner\")\n.join(schoolYearGradRate, Seq(\"School_BEDS_Code\", \"School_Name\", \"Year\"), \"inner\")\n.withColumn(\"School_BEDS_Code\", col(\"School_BEDS_Code\").cast(\"string\"))\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Combined Dataframe (Total: \" + combinedDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\ncombinedDF.printSchema\nz.show(combinedDF.limit(10))\n\n// save the dataframe into parquet\ncombinedDF.write.mode(\"overwrite\").parquet(root_folder + \"combined_cleaned.parquet\")\n\n// also save the dataframe into csv\ncombinedDF.write.mode(\"overwrite\").csv(root_folder + \"combined_cleaned.csv\")"
    }
  ]
}
