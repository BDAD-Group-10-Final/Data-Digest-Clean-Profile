{
  "metadata": {
    "name": "SCR Data Clean \u0026 Profile",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SCR Data Profile\n\u003e Kamiku Xue(yx3494@nyu.edu)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// define the project root\nval root_folder \u003d \"/user/yx3494_nyu_edu/scr_data/\"\nvar year \u003d 2018 to 2023"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1 BOCES and Need-to-Resource Capacity Categories(N/RC)\n\nThe need-to-resource capacity (N/RC) index, a measure of a district’s ability to meet the needs of its students with local\nresources, is the ratio of the estimated poverty percentage1 (expressed in standard score form) to the Combined Wealth\nRatio2 (expressed in standard score form). A district with both estimated poverty and Combined Wealth Ratio equal to\nthe State average would have a N/RC index of 1.0. N/RC categories are determined from this index using the definitions\nin the table below."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// First see the data schema\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n\n        println(\"Year \" + i)\n        println(\"Total Columns \" + df.columns.length)\n        \n        // see data structure\n        df.printSchema\n        \n        // peek some data\n        z.show(df.limit(10))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nWe will use the following columns:\n\n- `ENTITY_CD `: Unique identifier for the entity for foreign key\n- `SCHOOL_NAME`: The name of the school\n- `YEAR`: School Year (2021 for 2020-21, 2022 for 2021-22, 2023 for 2022-23)\n- `DISTRICT_NAME`: The name of the district\n- `COUNTY_NAME`: The name of the county\n- `NEEDS_INDEX`: N/RC index\n\nNow profile the each column"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// loop from 2018 to 2023\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n\n        println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n        println(\"Total entities \" + df.count)\n\n        // profile the YEAR\n        println(\"YEAR Profile\")\n        z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\")) // -\u003e 3, this explains some entities occur more than once, because they are in different years\n\n\n        // profile the ENTITY_CD\n        println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n        println(\"empty, null values: \" + df.filter(\"ENTITY_CD is null or ENTITY_CD \u003d \u0027\u0027\").count)\n        val groups_ids \u003d df.groupBy(\"ENTITY_CD\").count()\n        z.show(groups_ids.select(\"count\").describe()) // max: 3, min: 2, avg: 2.99 -\u003e most of the entities occur 3 times\n\n        // profile the SCHOOL_NAME\n        println(\"SCHOOL_NAME Profile, distinct schools:\" + df.select(\"SCHOOL_NAME\").distinct.count)\n        val groups_schools \u003d df.groupBy(\"SCHOOL_NAME\").count()\n        z.show(groups_schools.describe()) // some values occur 12 times, no empty names\n\n        // need to bind the SCHOOL_NAME with ENTITY_CD to check if the same school has different entity code\n        println(\"CHOOL_NAME + ENTITY_CD Profile, Distribution of entities\")\n        val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"SCHOOL_NAME\").count().select(\"count\")\n        z.show(groups_schools_entity.describe()) // max: 3, min: 2, avg: 2.99 -\u003e match the entity_cd\n\n        // profile the DISTRICT_NAME\n        println(\"DISTRICT_NAME Profile, unique districts: \" + df.select(\"DISTRICT_NAME\").distinct.count)\n        \n        //group by YEAR for the NEEDS_INDEX\n        val groups_districts \u003d df.groupBy(\"DISTRICT_NAME\").count()\n        z.show(groups_districts.describe()) //DISTRICT max occur 800, min 5 times\n\n        // profile the NEEDS_INDEX\n        println(\"NEEDS_INDEX Profile\")\n        z.show(df.describe(\"NEEDS_INDEX\")) // max:7, min: 1, avg: 3.55, stddev: 2.09\n        \n        //differset school year NEEDS_INDEX distribution\n        println(\"NEEDS_INDEX Profile in different years\")\n        z.show(df\n        .groupBy(\"YEAR\")\n        .agg(\n            sum(\"NEEDS_INDEX\"),\n            avg(\"NEEDS_INDEX\"), \n            min(\"NEEDS_INDEX\"), \n            max(\"NEEDS_INDEX\"), \n            stddev(\"NEEDS_INDEX\"))\n            .orderBy(\"YEAR\"))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n### N/RC Clean Step\n\nWe will do the following steps to clean the data"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// create a dataframe to store the data for all years\nvar nrcDF : DataFrame \u003d null\n\n// UDF for need index description\nval getNeedIndex \u003d (index: Int) \u003d\u003e {\n    index match {\n    case 1 \u003d\u003e \"High N/RC: New York City\"\n    case 2 \u003d\u003e \"High N/RC: Large City Districts \"\n    case 3 \u003d\u003e \"High N/RC: Urban-Suburban Districts\"\n    case 4 \u003d\u003e \"High N/RC: Rural Districts\"\n    case 5 \u003d\u003e \"Average N/RC Districts\"\n    case 6 \u003d\u003e \"Low N/RC Districts\"\n    case 7 \u003d\u003e \"Charter Schools\"\n    }\n}\nspark.udf.register(\"nrcStr\", getNeedIndex)\n\nfor (i \u003c- year){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/BOCES_NRC.csv\")\n        // filder other year\u0027s data\n        .filter(\"YEAR \u003d \" + i)\n        // drop null values in SCHOOL_NAME\n        .filter(\"SCHOOL_NAME is not null and SCHOOL_NAME !\u003d \u0027\u0027\")\n        // in DISTRICT_NAME, if null, replace with \u0027UNAVAILABLE\u0027\n        .withColumn(\"DISTRICT_NAME\", when(col(\"DISTRICT_NAME\").isNull, \"UNAVAILABLE\").otherwise(col(\"DISTRICT_NAME\")))\n        .withColumn(\"NEEDS_DESCRIPTION\", expr(\"nrcStr(NEEDS_INDEX)\"))\n        // only select the columns we need\n        .select(\"ENTITY_CD\", \"SCHOOL_NAME\", \"DISTRICT_NAME\", \"YEAR\", \"NEEDS_INDEX\", \"NEEDS_DESCRIPTION\")\n\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    z.show(df.limit(10))\n\n    if (i \u003d\u003d 2018){\n        nrcDF \u003d df\n    } else {\n        nrcDF \u003d nrcDF.union(df)\n    }\n}\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final NR/C Dataframe (Total: \" + nrcDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\nz.show(nrcDF.limit(10))\nz.show(nrcDF.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n\n// save the dataframe\nnrcDF.write.mode(\"overwrite\").parquet(root_folder + \"nrc_cleaned.parquet\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 2 Expenditures Data\n\nThe Expenditures data provides information on the financial resources available to schools and districts. The data is\nreported by school districts and BOCES.\n\n\u003e The Expenditures data only available starting from 2019."
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// First see all the schema\nfor (i \u003c- 2019 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Expenditures.csv\")\n        \n        println(\"Year \" + i)\n        println(\"Total Columns \" + df.columns.length)\n        // see data structure\n        df.printSchema\n        // peek some data\n        z.show(df.limit(10))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We plan to use the following columns:\n\n- `ENTITY_CD`: Unique identifier for the entity for foreign key\n- `ENTITY_NAME`: The name of the school / district\n- `YEAR`: School Year (2021 for 2020-21, 2022 for 2021-22, 2023 for 2022-23)\n- `PUPIL_COUNT_TOT`: Pupil counts for districts, schools, and statewide\n- `FEDERAL_EXP`: Total federal expenditures\n- `STATE_LOCAL_EXP`: Total state and local expenditures\n\nWe will not use the statistics columns, becuase we can calculate them from the data."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Start Profiling the Expenditures data\nfor (i \u003c- 2019 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Expenditures.csv\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // profile the YEAR\n    println(\"YEAR Profile Distribution\")\n    z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\")) // 2019 only have 1, other have 2 years\n    \n    // profile the ENTITY_CD\n    println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n    val groups_ids \u003d df.groupBy(\"ENTITY_CD\").count()\n    z.show(groups_ids.select(\"count\").describe()) // max: 2, min: 1, avg: 1.98 -\u003e most of the entities occur 2 years data\n\n    // profile the ENTITY_NAME + ENTITY_CD\n    println(\"ENTITY_NAME Profile, distinct schools:\" + df.select(\"ENTITY_NAME\").distinct.count)\n    val groups_schools \u003d df.groupBy(\"ENTITY_NAME\", \"ENTITY_CD\").count()\n    z.show(groups_schools.describe()) // some values occur 2 times, no empty names\n\n    // profile the PUPIL_COUNT_TOT\n    println(\"PUPIL_COUNT_TOT Profile\")\n    println(\"empty, null values: \" + df.filter(\"PUPIL_COUNT_TOT is null or PUPIL_COUNT_TOT \u003d 0\").count)\n    z.show(df.describe(\"PUPIL_COUNT_TOT\")) // max: 2.7M, min: 8, avg: 1.5K, stddev: 1.5K -\u003e some outliers\n    \n    // group by YEAR\n    println(\"PUPIL_COUNT_TOT Profile in different years\")\n    z.show(df.groupBy(\"YEAR\")\n        .agg(\n            sum(\"PUPIL_COUNT_TOT\"),\n            avg(\"PUPIL_COUNT_TOT\"), \n            min(\"PUPIL_COUNT_TOT\"), \n            max(\"PUPIL_COUNT_TOT\"), \n            stddev(\"PUPIL_COUNT_TOT\"))\n            .orderBy(\"YEAR\")) // the next year\u0027s data includes the previous year\u0027s data are the same\n\n\n    // profile FEDERAL_EXP and STATE_LOCAL_EXP\n    println(\"FEDERAL_EXP + STATE_LOCAL_EXP Profile\")\n    println(\"FEDERAL_EXP null values: \" + df.filter(\"FEDERAL_EXP is null\").count)\n    println(\"STATE_LOCAL_EXP null values: \" + df.filter(\"STATE_LOCAL_EXP is null\").count)\n\n    z.show(df.describe(\"FEDERAL_EXP\", \"STATE_LOCAL_EXP\"))\n\n    // distribution of FEDERAL_EXP\n    println(\"FEDERAL_EXP RANGE\")\n    val federal_exp_bins \u003d df.withColumn(\"FEDERAL_EXP_RANGE\", expr(\"CASE WHEN FEDERAL_EXP \u003c 1000 THEN \u0027\u003c 0\u0027 WHEN FEDERAL_EXP \u003c 1000 THEN \u00270 - 1K\u0027 WHEN FEDERAL_EXP \u003c 10000 THEN \u00271K - 10K\u0027 WHEN FEDERAL_EXP \u003c 100000 THEN \u002710K - 100K\u0027 WHEN FEDERAL_EXP \u003c 1000000 THEN \u0027100K - 1M\u0027 WHEN FEDERAL_EXP \u003c 10000000 THEN \u00271M - 10M\u0027 WHEN FEDERAL_EXP \u003c 100000000 THEN \u002710M - 100M\u0027 ELSE \u0027100M+\u0027 END\"))\n    z.show(federal_exp_bins.groupBy(\"FEDERAL_EXP_RANGE\").count().orderBy(\"count\"))\n\n    // distribution of STATE_LOCAL_EXP\n    println(\"STATE_LOCAL_EXP RANGE\")\n    val state_local_exp_bins \u003d df.withColumn(\"STATE_LOCAL_EXP_RANGE\", expr(\"CASE WHEN STATE_LOCAL_EXP \u003c 1000 THEN \u00270 - 1K\u0027 WHEN STATE_LOCAL_EXP \u003c 10000 THEN \u00271K - 10K\u0027 WHEN STATE_LOCAL_EXP \u003c 100000 THEN \u002710K - 100K\u0027 WHEN STATE_LOCAL_EXP \u003c 1000000 THEN \u0027100K - 1M\u0027 WHEN STATE_LOCAL_EXP \u003c 10000000 THEN \u00271M - 10M\u0027 WHEN STATE_LOCAL_EXP \u003c 100000000 THEN \u002710M - 100M\u0027 ELSE \u0027100M+\u0027 END\"))\n    z.show(state_local_exp_bins.groupBy(\"STATE_LOCAL_EXP_RANGE\").count().orderBy(\"count\"))\n\n    // combine the FEDERAL_EXP and STATE_LOCAL_EXP\n    println(\"FEDERAL_EXP + STATE_LOCAL_EXP\")\n    val total_exp \u003d df.withColumn(\"TOTAL_EXP\", $\"FEDERAL_EXP\" + $\"STATE_LOCAL_EXP\")\n    z.show(total_exp.describe(\"TOTAL_EXP\"))    \n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Expenditures Clean Step\n\nWe will do the following steps to clean the data:"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// create a dataframe to store the data for all years\nvar expDF : DataFrame \u003d null\n\n// Start Profiling the Expenditures data\nfor (i \u003c- 2019 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Expenditures.csv\")\n        // select the columns we need\n        .select(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\", \"PUPIL_COUNT_TOT\", \"FEDERAL_EXP\", \"STATE_LOCAL_EXP\")\n        // fill the FEDERAL_EXP and STATE_LOCAL_EXP with 0 if null\n        .withColumn(\"FEDERAL_EXP\", when(col(\"FEDERAL_EXP\").isNull, 0).otherwise(col(\"FEDERAL_EXP\")))\n        .withColumn(\"STATE_LOCAL_EXP\", when(col(\"STATE_LOCAL_EXP\").isNull, 0).otherwise(col(\"STATE_LOCAL_EXP\")))\n        // add the total exp\n        .withColumn(\"TOTAL_EXP\", $\"FEDERAL_EXP\" + $\"STATE_LOCAL_EXP\")\n        // select the current year\n        .filter(\"YEAR \u003d \" + i)\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    z.show(df.limit(10))\n\n    if (i \u003d\u003d 2019){\n        expDF \u003d df\n    } else {\n        expDF \u003d expDF.union(df)\n    }\n}\n\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final Expenditures Dataframe (Total: \" + expDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\nz.show(expDF.limit(10))\nz.show(expDF.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n\n// save the dataframe\nexpDF.write.mode(\"overwrite\").parquet(root_folder + \"exp_cleaned.parquet\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 3 Teaching Staff Data\n\nThe Teaching Staff data provides information on the number of teachers and principals have experience or inexperience in hight-proverty, low-performing schools.\n\n\u003e From 2020, the BOCES update the data version for Staff Qualiuifications, we need to check the columns when merge."
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2018 and 2019 have Staff_Qualifications.csv\nvar columns: Array[String] \u003d Array()\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // show the schema\n    println(\"Columns: \" + df.columns.length)\n    df.printSchema\n    // peek some data\n    z.show(df.limit(10))\n\n    if (columns.length \u003d\u003d 0){\n        columns \u003d df.columns\n    } else {\n        val diff \u003d df.columns.diff(columns)\n        if (diff.length \u003e 0){\n            println(\"Columns difference: \" + diff.mkString(\", \"))\n        }\n    }\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2020 - 2023 have Inexperienced_Teachers_Principals.csv\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    // show the schema\n    println(\"Columns: \" + df.columns.length)\n    df.printSchema\n\n    // check the difference columns\n    val diff \u003d df.columns.diff(columns)\n    if (diff.length \u003e 0){\n        println(\"Columns difference: \" + diff.mkString(\", \"))\n    }\n\n    // peek some data\n    z.show(df.limit(10))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We find some different columns in the teaching staff and qualifications data\n\n`INSTITUTION_ID, TOT_TEACH_LOW, TOT_TEACH_HIGH, TOT_PRINC_LOW, TOT_PRINC_HIGH, TEACH_DATA_REP_FLAG, PRIN_DATA_REP_FLAG`\n\nWe will not use above diff columns, we will use the following columns:\n\n- ENTITY_CD  - Unique identifier for the entity for foreign key\n- ENTITY_NAME - The name of the school / district\n- YEAR - School Year (etc. 2021 for 2020-21)\n- NUM_TEACH - Total number of teachers in the Student Information Repository System\n(SIRS)\n- NUM_TEACH_INEXP - Number of inexperienced teachers\n- NUM_TEACH_LOW - Number of teachers with low-poverty schools statewide\n- NUM_TEACH_HIGH - Number of teachers with high-poverty schools statewide\n- NUM_PRINC - Total number of principals\n- NUM_PRINC_INEXP - Number of inexperienced principals\n- NUM_ PRINC_LOW - Number of principals with low-poverty schools statewide\n- NUM_PRINC_HIGH - Number of principals with high-poverty schools statewide\n\nNext profile these data"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// for 2018 and 2019 Profile\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n    \n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    println(\"Total entities \" + df.count)    \n\n    // profile the YEAR\n    println(\"YEAR Profile\")\n    z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n\n    // profile the ENTITY_CD\n    println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n\n    // profile the ENTITY_NAME\n    println(\"ENTITY_NAME Profile, distinct schools:\" + df.select(\"ENTITY_NAME\").distinct.count)\n\n    // profile the ENTITY_CD + ENTITY_NAME\n    println(\"ENTITY_CD + ENTITY_NAME Profile, Distribution of entities\")\n    val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"ENTITY_NAME\").count().select(\"count\")\n    z.show(groups_schools_entity.describe())\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH Profile\")\n    z.show(df.describe(\"NUM_TEACH\")) \n\n    // profile the NUM_TEACH_INEXP\n    println(\"NUM_TEACH_INEXP Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH_LOW Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_PRINC\n    println(\"NUM_PRINC Profile\")\n    z.show(df.describe(\"NUM_PRINC\"))\n\n    // profile the NUM_PRINC_INEXP\n    println(\"NUM_PRINC_INEXP Profile\")\n    z.show(df.describe(\"NUM_PRINC_INEXP\"))\n\n    // profile the NUM_PRINC_LOW\n    println(\"NUM_PRINC_LOW Profile\")\n    z.show(df.describe(\"NUM_PRINC_LOW\"))\n\n    // profile the NUM_PRINC_HIGH\n    println(\"NUM_PRINC_HIGH Profile\")\n    z.show(df.describe(\"NUM_PRINC_HIGH\"))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// for 2020 and 2023 Inexperienced Teachers Principals\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n    \n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    println(\"Total entities \" + df.count)    \n\n    // profile the YEAR\n    println(\"YEAR Profile\")\n    z.show(df.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n    // profile the ENTITY_CD\n    println(\"ENTITY_CD Profile, distinct entities: \" + df.select(\"ENTITY_CD\").distinct.count)\n\n    // profile the ENTITY_NAME\n    println(\"ENTITY_NAME Profile, distinct schools:\" + df.select(\"ENTITY_NAME\").distinct.count)\n\n    // profile the ENTITY_CD + ENTITY_NAME\n    println(\"ENTITY_CD + ENTITY_NAME Profile, Distribution of entities\")\n    val groups_schools_entity \u003d df.groupBy(\"ENTITY_CD\", \"ENTITY_NAME\").count().select(\"count\")\n    z.show(groups_schools_entity.describe())\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH Profile\")\n    z.show(df.describe(\"NUM_TEACH\")) \n\n    // profile the NUM_TEACH_INEXP\n    println(\"NUM_TEACH_INEXP Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_TEACH\n    println(\"NUM_TEACH_LOW Profile\")\n    z.show(df.describe(\"NUM_TEACH_INEXP\"))\n\n    // profile the NUM_PRINC\n    println(\"NUM_PRINC Profile\")\n    z.show(df.describe(\"NUM_PRINC\"))\n\n    // profile the NUM_PRINC_INEXP\n    println(\"NUM_PRINC_INEXP Profile\")\n    z.show(df.describe(\"NUM_PRINC_INEXP\"))\n\n    // profile the NUM_PRINC_LOW\n    println(\"NUM_PRINC_LOW Profile\")\n    z.show(df.describe(\"NUM_PRINC_LOW\"))\n\n    // profile the NUM_PRINC_HIGH\n    println(\"NUM_PRINC_HIGH Profile\")\n    z.show(df.describe(\"NUM_PRINC_HIGH\"))\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Teaching Staff Clean Step\n\nThis data is almost clean, just pick the columns needed and merge to one table"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// create a dataframe to store the data for 2018 and 2019\nvar oldStaffDF : DataFrame \u003d null\n\n// 2018 and 2019 have Staff_Qualifications.csv\nfor (i \u003c- 2018 to 2019){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Staff_Qualifications.csv\")\n        // select the current year data\n        .filter(\"YEAR \u003d \" + i)\n        // select the columns\n        .select(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\", \"NUM_TEACH\", \"NUM_TEACH_INEXP\", \"NUM_TEACH_LOW\", \"NUM_PRINC\", \"NUM_PRINC_INEXP\", \"NUM_PRINC_LOW\", \"NUM_PRINC_HIGH\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    z.show(df.limit(10))\n\n    if (oldStaffDF \u003d\u003d null){\n        oldStaffDF \u003d df\n    } else {\n        oldStaffDF \u003d oldStaffDF.union(df)\n    }\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// create a dataframe to store the data for 2018 and 2019\nvar newStaffDF : DataFrame \u003d null\n// 2020 and 2023 have Inexperienced_Teachers_Principals.csv\nfor (i \u003c- 2020 to 2023){\n    var df \u003d spark.read.option(\"header\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(root_folder + i + \"/Inexperienced_Teachers_Principals.csv\")\n        // select the current year data\n        .filter(\"YEAR \u003d \" + i)\n        // select the columns\n        .select(\"ENTITY_CD\", \"ENTITY_NAME\", \"YEAR\", \"NUM_TEACH\", \"NUM_TEACH_INEXP\", \"NUM_TEACH_LOW\", \"NUM_PRINC\", \"NUM_PRINC_INEXP\", \"NUM_PRINC_LOW\", \"NUM_PRINC_HIGH\")\n\n    println(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Year \" + i + \" (Total: \" + df.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n    z.show(df.limit(10))\n    \n    // union(merge) the dataframes\n    if (newStaffDF \u003d\u003d null){\n        newStaffDF \u003d df\n    } else {\n        newStaffDF \u003d newStaffDF.union(df)\n    }\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// final staff dataframe for 2018 - 2023\nval finalStaffDF \u003d oldStaffDF.union(newStaffDF)\nprintln(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Final Staff Dataframe (Total: \" + finalStaffDF.count + \") \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\n// peek some data row\nz.show(finalStaffDF.limit(10))\n// show the year distribution\nz.show(finalStaffDF.groupBy(\"YEAR\").count().orderBy(\"YEAR\"))\n// save the dataframe\nfinalStaffDF.write.mode(\"overwrite\").parquet(root_folder + \"staff_cleaned.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}