{
  "metadata": {
    "name": "school_analysis.ipynb",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filePath \u003d \"/user/yx3494_nyu_edu/scr_data/funding_safety.parquet\"\nval df \u003d spark.read.parquet(filePath)\n    \nz.show(df)\n\ndf.createOrReplaceTempView(\"all\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val temp1 \u003d spark.sql(\"\"\"\n    select count(distinct(School_BEDS_Code))\n    from all\n\"\"\")\n\nz.show(temp1)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val safetyIssueColumns \u003d df.columns.slice(5,15)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val fundingSafetyDF \u003d df.withColumn(\n  \"Sum_Safety_Issues\",\n  safetyIssueColumns.map(colName \u003d\u003e col(colName)).reduce(_ + _)\n)\n\nz.show(fundingSafetyDF)\n\nfundingSafetyDF.createOrReplaceTempView(\"fundingSafety\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Step 1: Calculate min and max for each column\nval stats \u003d fundingSafetyDF.agg(\n  min(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_min\"),\n  max(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_max\"),\n  min(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_min\"),\n  max(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_max\")\n).collect()(0)\n\n// Extract min and max values as scalars\nval totalFundingMin \u003d stats.getAs[Double](\"Total_Funding_min\")\nval totalFundingMax \u003d stats.getAs[Double](\"Total_Funding_max\")\nval sumSafetyIssuesMin \u003d stats.getAs[Double](\"Sum_Safety_Issues_min\")\nval sumSafetyIssuesMax \u003d stats.getAs[Double](\"Sum_Safety_Issues_max\")\n\n// Step 2: Normalize the columns using Min-Max Normalization\nval normalizedData \u003d fundingSafetyDF\n  .withColumn(\"Total_Funding_normalized\", \n    (col(\"Total_Funding\").cast(\"double\") - lit(totalFundingMin)) / lit(totalFundingMax - totalFundingMin))\n  .withColumn(\"Sum_Safety_Issues_normalized\", \n    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin)) / lit(sumSafetyIssuesMax - sumSafetyIssuesMin))\n\n// Step 3: Compute correlation between normalized columns\nval correlation \u003d normalizedData.stat.corr(\"Total_Funding_normalized\", \"Sum_Safety_Issues_normalized\")\n\n// Print the correlation\nprintln(s\"Correlation between Total_Funding and Sum_Safety_Issues: $correlation\")\n\n// Show normalized data if needed\nz.show(normalizedData)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filePath2 \u003d \"/user/yx3494_nyu_edu/scr_data/funding_safety_nrc_inexp_gradRate.parquet\"\nval df2 \u003d spark.read.parquet(filePath2)\n    \nz.show(df2)\n\ndf2.createOrReplaceTempView(\"all2\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val temp2 \u003d spark.sql(\"\"\"\n    select count(distinct(School_BEDS_Code))\n    from all2\n\"\"\")\n\nz.show(temp2)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val fundingSafetyGradDF \u003d df2.withColumn(\n  \"Sum_Safety_Issues\",\n  safetyIssueColumns.map(colName \u003d\u003e col(colName)).reduce(_ + _))\n\nz.show(fundingSafetyGradDF)\n\nfundingSafetyGradDF.createOrReplaceTempView(\"fundingSafetyGrad\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val temp3 \u003d spark.sql(\"\"\"\n    select *\n    from fundingSafetyGrad\n    where Graduation_Rate \u003c 40\n\"\"\")\n\nz.show(temp3)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Step 1: Calculate min and max for each column\nval stats2 \u003d fundingSafetyGradDF.agg(\n  min(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_min\"),\n  max(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_max\"),\n  min(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_min\"),\n  max(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_max\")\n).collect()(0)\n\n// Extract min and max values as scalars\nval totalFundingMin2 \u003d stats2.getAs[Double](\"Total_Funding_min\")\nval totalFundingMax2 \u003d stats2.getAs[Double](\"Total_Funding_max\")\nval sumSafetyIssuesMin2 \u003d stats2.getAs[Double](\"Sum_Safety_Issues_min\")\nval sumSafetyIssuesMax2 \u003d stats2.getAs[Double](\"Sum_Safety_Issues_max\")\n\n// Step 2: Normalize the columns using Min-Max Normalization\nval normalizedData2 \u003d fundingSafetyGradDF\n  .withColumn(\"Total_Funding_normalized\", \n    (col(\"Total_Funding\").cast(\"double\") - lit(totalFundingMin2)) / lit(totalFundingMax2 - totalFundingMin2))\n  .withColumn(\"Sum_Safety_Issues_normalized\", \n    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin2)) / lit(sumSafetyIssuesMax2 - sumSafetyIssuesMin2))\n\n// Step 3: Compute correlation between normalized columns\nval correlation2 \u003d normalizedData2.stat.corr(\"Total_Funding_normalized\", \"Sum_Safety_Issues_normalized\")\n\n// Print the correlation\nprintln(s\"Correlation between Total_Funding and Sum_Safety_Issues: ${correlation2}\")\n\n// Show normalized data if needed\nz.show(normalizedData2)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\nval stats3 \u003d fundingSafetyGradDF.agg(\n  min(col(\"Graduation_Rate\").cast(\"double\")).alias(\"Graduation_Rate_min\"),\n  max(col(\"Graduation_Rate\").cast(\"double\")).alias(\"Graduation_Rate_max\")\n).collect()(0)\n\nval graduationRateMin \u003d stats3.getAs[Double](\"Graduation_Rate_min\")\nval graduationRateMax \u003d stats3.getAs[Double](\"Graduation_Rate_max\")\n\nval normalizedData3 \u003d fundingSafetyGradDF\n  .withColumn(\"Graduation_Rate_normalized\", \n    (col(\"Graduation_Rate\").cast(\"double\") - lit(graduationRateMin)) / lit(graduationRateMax - graduationRateMin))\n  .withColumn(\"Sum_Safety_Issues_normalized\", \n    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin2)) / lit(sumSafetyIssuesMax2 - sumSafetyIssuesMin2))\n\nval correlation3 \u003d normalizedData3.stat.corr(\"Graduation_Rate_normalized\", \"Sum_Safety_Issues_normalized\")\n\nprintln(s\"Correlation between Graduation_Rate and Sum_Safety_Issues: ${correlation3}\")\n\nz.show(normalizedData3)"
    }
  ]
}