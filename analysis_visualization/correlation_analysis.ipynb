{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val filePath = \"/user/yx3494_nyu_edu/scr_data/funding_safety.parquet\"\n",
    "val df = spark.read.parquet(filePath)\n",
    "    \n",
    "z.show(df)\n",
    "\n",
    "df.createOrReplaceTempView(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val temp1 = spark.sql(\"\"\"\n",
    "    select count(distinct(School_BEDS_Code))\n",
    "    from all\n",
    "\"\"\")\n",
    "\n",
    "z.show(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val safetyIssueColumns = df.columns.slice(5,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val fundingSafetyDF = df.withColumn(\n",
    "  \"Sum_Safety_Issues\",\n",
    "  safetyIssueColumns.map(colName => col(colName)).reduce(_ + _)\n",
    ")\n",
    "\n",
    "z.show(fundingSafetyDF)\n",
    "\n",
    "fundingSafetyDF.createOrReplaceTempView(\"fundingSafety\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Step 1: Calculate min and max for each column\n",
    "val stats = fundingSafetyDF.agg(\n",
    "  min(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_min\"),\n",
    "  max(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_max\"),\n",
    "  min(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_min\"),\n",
    "  max(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_max\")\n",
    ").collect()(0)\n",
    "\n",
    "// Extract min and max values as scalars\n",
    "val totalFundingMin = stats.getAs[Double](\"Total_Funding_min\")\n",
    "val totalFundingMax = stats.getAs[Double](\"Total_Funding_max\")\n",
    "val sumSafetyIssuesMin = stats.getAs[Double](\"Sum_Safety_Issues_min\")\n",
    "val sumSafetyIssuesMax = stats.getAs[Double](\"Sum_Safety_Issues_max\")\n",
    "\n",
    "// Step 2: Normalize the columns using Min-Max Normalization\n",
    "val normalizedData = fundingSafetyDF\n",
    "  .withColumn(\"Total_Funding_normalized\", \n",
    "    (col(\"Total_Funding\").cast(\"double\") - lit(totalFundingMin)) / lit(totalFundingMax - totalFundingMin))\n",
    "  .withColumn(\"Sum_Safety_Issues_normalized\", \n",
    "    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin)) / lit(sumSafetyIssuesMax - sumSafetyIssuesMin))\n",
    "\n",
    "// Step 3: Compute correlation between normalized columns\n",
    "val correlation = normalizedData.stat.corr(\"Total_Funding_normalized\", \"Sum_Safety_Issues_normalized\")\n",
    "\n",
    "// Print the correlation\n",
    "println(s\"Correlation between Total_Funding and Sum_Safety_Issues: $correlation\")\n",
    "\n",
    "// Show normalized data if needed\n",
    "z.show(normalizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val filePath2 = \"/user/yx3494_nyu_edu/scr_data/funding_safety_nrc_gradRate.parquet\"\n",
    "val df2 = spark.read.parquet(filePath2)\n",
    "    \n",
    "z.show(df2)\n",
    "\n",
    "df2.createOrReplaceTempView(\"all2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val temp2 = spark.sql(\"\"\"\n",
    "    select count(distinct(School_BEDS_Code))\n",
    "    from all2\n",
    "\"\"\")\n",
    "\n",
    "z.show(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val fundingSafetyGradDF = df2.withColumn(\n",
    "  \"Sum_Safety_Issues\",\n",
    "  safetyIssueColumns.map(colName => col(colName)).reduce(_ + _)\n",
    ").withColumnRenamed(\"Gruadation_Rate\", \"Graduation_Rate\")\n",
    "\n",
    "\n",
    "z.show(fundingSafetyGradDF)\n",
    "\n",
    "fundingSafetyDF.createOrReplaceTempView(\"fundingSafetyGrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Step 1: Calculate min and max for each column\n",
    "val stats2 = fundingSafetyGradDF.agg(\n",
    "  min(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_min\"),\n",
    "  max(col(\"Total_Funding\").cast(\"double\")).alias(\"Total_Funding_max\"),\n",
    "  min(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_min\"),\n",
    "  max(col(\"Sum_Safety_Issues\").cast(\"double\")).alias(\"Sum_Safety_Issues_max\")\n",
    ").collect()(0)\n",
    "\n",
    "// Extract min and max values as scalars\n",
    "val totalFundingMin2 = stats2.getAs[Double](\"Total_Funding_min\")\n",
    "val totalFundingMax2 = stats2.getAs[Double](\"Total_Funding_max\")\n",
    "val sumSafetyIssuesMin2 = stats2.getAs[Double](\"Sum_Safety_Issues_min\")\n",
    "val sumSafetyIssuesMax2 = stats2.getAs[Double](\"Sum_Safety_Issues_max\")\n",
    "\n",
    "// Step 2: Normalize the columns using Min-Max Normalization\n",
    "val normalizedData2 = fundingSafetyGradDF\n",
    "  .withColumn(\"Total_Funding_normalized\", \n",
    "    (col(\"Total_Funding\").cast(\"double\") - lit(totalFundingMin2)) / lit(totalFundingMax2 - totalFundingMin2))\n",
    "  .withColumn(\"Sum_Safety_Issues_normalized\", \n",
    "    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin2)) / lit(sumSafetyIssuesMax2 - sumSafetyIssuesMin2))\n",
    "\n",
    "// Step 3: Compute correlation between normalized columns\n",
    "val correlation2 = normalizedData2.stat.corr(\"Total_Funding_normalized\", \"Sum_Safety_Issues_normalized\")\n",
    "\n",
    "// Print the correlation\n",
    "println(s\"Correlation between Total_Funding and Sum_Safety_Issues: ${correlation2}\")\n",
    "\n",
    "// Show normalized data if needed\n",
    "z.show(normalizedData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val stats3 = fundingSafetyGradDF.agg(\n",
    "  min(col(\"Graduation_Rate\").cast(\"double\")).alias(\"Graduation_Rate_min\"),\n",
    "  max(col(\"Graduation_Rate\").cast(\"double\")).alias(\"Graduation_Rate_max\")\n",
    ").collect()(0)\n",
    "\n",
    "val graduationRateMin = stats3.getAs[Double](\"Graduation_Rate_min\")\n",
    "val graduationRateMax = stats3.getAs[Double](\"Graduation_Rate_max\")\n",
    "\n",
    "val normalizedData3 = fundingSafetyGradDF\n",
    "  .withColumn(\"Graduation_Rate_normalized\", \n",
    "    (col(\"Graduation_Rate\").cast(\"double\") - lit(graduationRateMin)) / lit(graduationRateMax - graduationRateMin))\n",
    "  .withColumn(\"Sum_Safety_Issues_normalized\", \n",
    "    (col(\"Sum_Safety_Issues\").cast(\"double\") - lit(sumSafetyIssuesMin2)) / lit(sumSafetyIssuesMax2 - sumSafetyIssuesMin2))\n",
    "\n",
    "val correlation3 = normalizedData3.stat.corr(\"Graduation_Rate_normalized\", \"Sum_Safety_Issues_normalized\")\n",
    "\n",
    "println(s\"Correlation between Graduation_Rate and Sum_Safety_Issues: ${correlation3}\")\n",
    "\n",
    "z.show(normalizedData3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "name": "school_analysis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
