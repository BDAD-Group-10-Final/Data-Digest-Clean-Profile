{
  "metadata": {
    "name": "safety_data_ingestion.ipynb",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Ingestion\n\nThe safety dataset is from [IRS SCHOOL SAFETY AND THE EDUCATIONAL CLIMATE](https://www.p12.nysed.gov/irs/school_safety/school_safety_data_reporting.htmll).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The datasets can be directly downloaded from the website. Then the datasets are uploaded to the data ingest website. And then transferred to the HDFS with the following code.\n\nhadoop distcp gs://nyu-dataproc-hdfs-ingest/safety_data /user/yz6956_nyu_edu/project"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Cleaning"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n\nval schema1 \u003d StructType(Array(\n  StructField(\"County\", StringType, true),\n  StructField(\"District\", StringType, true),\n  StructField(\"School_Name\", StringType, true),\n  StructField(\"BEDS_Code\", StringType, true),\n  StructField(\"Grade_Organization\", StringType, true),\n  StructField(\"Need/Resource_Category\", StringType, true),\n  StructField(\"School_Type\", StringType, true),\n  StructField(\"Enrollment\", IntegerType, true),\n  StructField(\"Inactive_Date\", StringType, true),\n  StructField(\"Homocide\", IntegerType, true),\n  StructField(\"Sexual_Offenses-Forcible_Sex\", IntegerType, true),\n  StructField(\"Sexual_Offenses-Other\", IntegerType, true),\n  StructField(\"Assault-Physical_Injury\", IntegerType, true),\n  StructField(\"Assault-Serious_Physical_Injury\", IntegerType, true),\n  StructField(\"Weapons_Possession-Routine_Security_Check\", IntegerType, true),\n  StructField(\"Weapons_Possession-Other\", IntegerType, true),\n  StructField(\"Dignity Act-Excluding_Cyberbullying\", IntegerType, true),\n  StructField(\"Dignity Act-Cyberbullying\", IntegerType, true),\n  StructField(\"Bomb_Threat\", IntegerType, true),\n  StructField(\"False_Alarm\", IntegerType, true),\n  StructField(\"Drugs\", IntegerType, true),\n  StructField(\"Alcohol\", IntegerType, true)\n))\n\nval schema2 \u003d StructType(Array(\n  StructField(\"County\", StringType, true),\n  StructField(\"District\", StringType, true),\n  StructField(\"School_Name\", StringType, true),\n  StructField(\"BEDS_Code\", StringType, true),\n  StructField(\"Grade_Organization\", StringType, true),\n  StructField(\"Need/Resource_Category\", StringType, true),\n  StructField(\"School_Type\", StringType, true),\n  StructField(\"Enrollment\", IntegerType, true),\n  StructField(\"Inactive_Date\", StringType, true),\n  StructField(\"Homocide\", IntegerType, true),\n  StructField(\"Sexual_Offense\", IntegerType, true),\n  StructField(\"Assault\", IntegerType, true),\n  StructField(\"Weapons_Possession\", IntegerType, true),\n  StructField(\"Dignity Act-Excluding_Cyberbullying\", IntegerType, true),\n  StructField(\"Dignity Act-Cyberbullying\", IntegerType, true),\n  StructField(\"Bomb_Threat\", IntegerType, true),\n  StructField(\"False_Alarm\", IntegerType, true),\n  StructField(\"Drugs\", IntegerType, true),\n  StructField(\"Alcohol\", IntegerType, true),\n  StructField(\"Other_Threats\", IntegerType, true)\n))"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\ndef clean1(filePath: String) \u003d {\n  // read the CSV file\n  val rawDF \u003d spark.read\n    .option(\"multiLine\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .schema(schema1)\n    .csv(filePath)\n    .rdd\n    .zipWithIndex()\n    .filter { case (_, index) \u003d\u003e index \u003e\u003d 3 }\n    .map(_._1)\n\n  // combine subsectors into one\n  // drop the unused columns\n  // filter out the inactive schools\n  val df_temp \u003d spark.createDataFrame(rawDF, schema1)\n    .filter($\"Inactive_Date\".isNull)\n    .withColumn(\"Sexual_Offense\", col(\"Sexual_Offenses-Forcible_Sex\") + col(\"Sexual_Offenses-Other\"))\n    .withColumn(\"Assault\", col(\"Assault-Physical_Injury\") + col(\"Assault-Serious_Physical_Injury\"))\n    .withColumn(\"Weapons_Possession\", col(\"Weapons_Possession-Routine_Security_Check\") + col(\"Weapons_Possession-Other\"))\n    .drop(\"Grade_Organization\", \"Need/Resource_Category\", \"Inactive_Date\", \"Sexual_Offenses-Forcible_Sex\", \"Sexual_Offenses-Other\")\n    .drop(\"Assault-Physical_Injury\", \"Assault-Serious_Physical_Injury\", \"Weapons_Possession-Routine_Security_Check\", \"Weapons_Possession-Other\")\n  \n  val columns \u003d df_temp.columns.slice(0, 6) ++ Seq(\"Sexual_Offense\") ++ Seq(\"Assault\") ++ Seq(\"Weapons_Possession\") ++ df_temp.columns.slice(6, 13)\n  val df \u003d df_temp.select(columns.map(df_temp.col): _*)\n\n  df\n}\n\ndef clean2(filePath: String) \u003d {\n  // read the CSV file\n  val rawDF \u003d spark.read\n    .option(\"multiLine\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .schema(schema2)\n    .csv(filePath)\n    .rdd\n    .zipWithIndex()\n    .filter { case (_, index) \u003d\u003e index \u003e\u003d 3 }\n    .map(_._1)\n\n  // drop the unused columns\n  // filter out the inactive schools\n  val df \u003d spark.createDataFrame(rawDF, schema2).filter($\"Inactive_Date\".isNull).drop(\"Grade_Organization\", \"Need/Resource_Category\", \"Inactive_Date\", \"Other_Threats\")\n\n  df\n}\n\ndef combine1(filePath1: String, filePath2: String) \u003d {\n  val df1 \u003d clean1(filePath1)\n  val df2 \u003d clean1(filePath2)\n  df1.union(df2)\n}\n\ndef combine2(filePath1: String, filePath2: String) \u003d {\n  val df1 \u003d clean2(filePath1)\n  val df2 \u003d clean2(filePath2)\n  df1.union(df2)\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val year \u003d \"2018\"\n\nval filePath1 \u003d s\"project/safety_data/${year}_SSEC_NYC.csv\"\nval filePath2 \u003d s\"project/safety_data/${year}_SSEC_ROS.csv\"\nval df2018 \u003d combine1(filePath1, filePath2)\n\nz.show(df2018)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val year \u003d \"2022\"\n\nval filePath1 \u003d s\"project/safety_data/${year}_SSEC_NYC.csv\"\nval filePath2 \u003d s\"project/safety_data/${year}_SSEC_ROS.csv\"\nval df2022 \u003d combine2(filePath1, filePath2)\n\nz.show(df2022)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df2018.columns.length \u003d\u003d df2022.columns.length"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// 2018-2021, 2022-2023 datasets have different schema\n// Use different cleaning functions for different schema\n\nfor (year \u003c- 2018 to 2021) {\n    val filePath1 \u003d s\"project/safety_data/${year}_SSEC_NYC.csv\"\n    val filePath2 \u003d s\"project/safety_data/${year}_SSEC_ROS.csv\"\n    val df \u003d combine1(filePath1, filePath2)\n\n    val outputPath \u003d s\"project/cleaned_data/safety${year}.csv\"\n\n    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(outputPath)\n}\n\nfor (year \u003c- 2022 to 2023) {\n    val filePath1 \u003d s\"project/safety_data/${year}_SSEC_NYC.csv\"\n    val filePath2 \u003d s\"project/safety_data/${year}_SSEC_ROS.csv\"\n    val df \u003d combine2(filePath1, filePath2)\n\n    val outputPath \u003d s\"project/cleaned_data/safety${year}.csv\"\n\n    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(outputPath)\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Profiling"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Per Year Profiling"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val cleanFilePath \u003d \"project/cleaned_data/safety2022.csv\"\n\nval safety2022 \u003d spark.read\n    .option(\"multiLine\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .option(\"header\", true)\n    .csv(cleanFilePath)\n\nz.show(safety2022)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "for (year \u003c- 2018 to 2023) {\n    val cleanFilePath \u003d s\"project/cleaned_data/safety${year}.csv\"\n    val safetydf \u003d spark.read\n    .option(\"multiLine\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .option(\"header\", true)\n    .csv(cleanFilePath)\n    \n    // create a dataframe for the current year\n    safetydf.createOrReplaceTempView(s\"safety$year\")\n    \n    println(s\"Year: ${year} | Number of Schools / Rows count: ${safetydf.count()}\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val threatColumns \u003d Seq(\n  \"Homocide\",\n  \"Sexual_Offense\",\n  \"Assault\",\n  \"Weapons_Possession\",\n  \"Dignity Act-Excluding_Cyberbullying\",\n  \"Dignity Act-Cyberbullying\",\n  \"Bomb_Threat\",\n  \"False_Alarm\",\n  \"Drugs\",\n  \"Alcohol\"\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.DataFrame\n\nvar totalThreatDF: DataFrame \u003d spark.emptyDataFrame\nvar avgThreatDF: DataFrame \u003d spark.emptyDataFrame\n\nfor (year \u003c- 2018 to 2023) {\n    val cleanFilePath \u003d s\"project/cleaned_data/safety${year}.csv\"\n    val safetydf \u003d spark.read\n    .option(\"multiLine\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .option(\"header\", true)\n    .csv(cleanFilePath)\n    \n    // create a dataframe for the current year\n    safetydf.createOrReplaceTempView(s\"safety$year\")\n    \n    val count \u003d safetydf.count()\n    \n    // sum the number of each violations\n    val rawtotal \u003d spark.sql(s\"\"\"\n        select ${threatColumns.map(c \u003d\u003e s\"SUM(`$c`) as `$c`\").mkString(\", \")}\n        from safety${year}\n    \"\"\").withColumn(\"Year\", lit(year))\n    \n    // sum the number of each violations / number of schools\n    val rawavg \u003d spark.sql(s\"\"\"\n        select ${threatColumns.map(c \u003d\u003e s\"SUM(`$c`)/$count as `$c`\").mkString(\", \")}\n        from safety${year}\n    \"\"\").withColumn(\"Year\", lit(year))\n    \n    // add the year column to the dataframe\n    val columns \u003d Seq(\"Year\") ++ rawtotal.columns.slice(0, 11)\n    val total \u003d rawtotal.select(columns.map(rawtotal.col): _*)\n    val avg \u003d rawavg.select(columns.map(rawavg.col): _*)\n        \n    // add the current-year dataframe to the overall dataframe\n    totalThreatDF \u003d if (totalThreatDF.isEmpty) total else totalThreatDF.union(total)\n    avgThreatDF \u003d if (avgThreatDF.isEmpty) avg else avgThreatDF.union(avg)\n}\n\nz.show(totalThreatDF)\nz.show(avgThreatDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "avgThreatDF.createOrReplaceTempView(\"avgThreat\")\n\nval avgYoYDF \u003d spark.sql(\"\"\"\n  select\n    cur.Year,\n    round((cur.Homocide - prev.Homocide) / prev.Homocide * 100, 2) AS `Homocide_YoY (%)`,\n    round((cur.Sexual_Offense - prev.Sexual_Offense) / prev.Sexual_Offense * 100, 2) AS `Sexual_Offense_YoY (%)`,\n    round((cur.Assault - prev.Assault) / prev.Assault * 100, 2) AS `Assault_YoY (%)`,\n    round((cur.Weapons_Possession - prev.Weapons_Possession) / prev.Weapons_Possession * 100, 2) AS `Weapons_Possession_YoY (%)`,\n    round((cur.`Dignity Act-Excluding_Cyberbullying` - prev.`Dignity Act-Excluding_Cyberbullying`) / prev.`Dignity Act-Excluding_Cyberbullying` * 100, 2) AS `Dignity_Act_Excluding_Cyberbullying_YoY (%)`,\n    round((cur.`Dignity Act-Cyberbullying` - prev.`Dignity Act-Cyberbullying`) / prev.`Dignity Act-Cyberbullying` * 100, 2) AS `Dignity_Act_Cyberbullying_YoY (%)`,\n    round((cur.Bomb_Threat - prev.Bomb_Threat) / prev.Bomb_Threat * 100, 2) AS `Bomb_Threat_YoY (%)`,\n    round((cur.False_Alarm - prev.False_Alarm) / prev.False_Alarm * 100, 2) AS `False_Alarm_YoY (%)`,\n    round((cur.Drugs - prev.Drugs) / prev.Drugs * 100, 2) AS `Drugs_YoY (%)`,\n    round((cur.Alcohol - prev.Alcohol) / prev.Alcohol * 100, 2) AS `Alcohol_YoY (%)`\n  from\n    avgThreat cur\n  join\n    avgThreat prev\n  on\n    cur.Year \u003d prev.Year + 1\n\"\"\")\n\nz.show(avgYoYDF)\n"
    }
  ]
}